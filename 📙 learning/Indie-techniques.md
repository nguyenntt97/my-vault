#indie #notes 

Summary of various techniques that I found during my research

# Terminology demystifying

## 1. Embedding vs Encoding

-  <u>Answer:</u> Embedding suggests it is trained; see [@ref](https://ai.stackexchange.com/questions/37021/which-positional-encoding-bert-use)
## 2. Feature vs Descriptor

- 

# Teacher forcing
---



# Masking
---
[[Transformer]]


# Positional Encoding

## 1. Sinusoidal Functions

> Why Are Sinusoidal Functions Used for Position Encoding?

1. https://www.reddit.com/r/learnmachinelearning/comments/12h0veg/d_why_are_sinusoidal_functions_used_for_position/
2. https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/exs7d08/

## 2. 

# Sequence-to-Sequence

Girin, L., Leglaive, S., Bie, X., Diard, J., Hueber, T., & Alameda-Pineda, X. (2021). Dynamical Variational Autoencoders: A Comprehensive Review. _Foundations and Trends® in Machine Learning_, _15_(1–2), 1–175. [https://doi.org/10.1561/2200000089](https://doi.org/10.1561/2200000089)

